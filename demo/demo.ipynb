{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11ed5cec",
      "metadata": {},
      "source": [
        "# SBERT MR Demo (Jittor)\n",
        "\n",
        "This notebook downloads the MR (movie review) dataset, trains SBERTJittor on MR, and prints the best dev accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc23623e",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "Imports and CUDA setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e40ded",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import jittor as jt\n",
        "\n",
        "jt.flags.use_cuda = 1 if jt.has_cuda else 0\n",
        "print('CUDA:', bool(jt.flags.use_cuda))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d342680",
      "metadata": {},
      "source": [
        "## 2. Download MR dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acba3fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_dir = '../data' # set as yours\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "print('Downloading MR (rotten_tomatoes)...')\n",
        "mr = load_dataset('rotten_tomatoes')\n",
        "mr.save_to_disk(os.path.join(data_dir, 'MR'))\n",
        "print('MR saved to', os.path.join(data_dir, 'MR'))\n",
        "print('Train:', len(mr['train']), 'Val:', len(mr['validation']), 'Test:', len(mr['test']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1611e44b",
      "metadata": {},
      "source": [
        "## 3. Configure paths\n",
        "Set model name and optional encoder checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f823341c",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = 'roberta-base'\n",
        "pooling = 'mean'\n",
        "data_dir = '../data'\n",
        "cache_dir = '../data/tokenized'\n",
        "\n",
        "# Jittor checkpoint from a trained sroberta model\n",
        "jittor_ckpt = './important_checkpoints/nli/sroberta-base_best.pkl' # change this to your route\n",
        "\n",
        "output_dir = './checkpoints/mr_demo'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c20d116",
      "metadata": {},
      "source": [
        "## 4. Train on MR\n",
        "Runs the MR training script and saves the best checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a29579c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datasets import load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import jittor as jt\n",
        "from jittor import nn\n",
        "\n",
        "from model.sbert_model import SBERTJittor\n",
        "\n",
        "def _jt_array(data, dtype):\n",
        "    return jt.array(np.asarray(data, dtype=dtype))\n",
        "\n",
        "def _to_batch(batch):\n",
        "    out = {\n",
        "        'input_ids': _jt_array(batch['input_ids'], 'int32'),\n",
        "        'attention_mask': _jt_array(batch['attention_mask'], 'float32'),\n",
        "        'labels': _jt_array(batch['labels'], 'int32'),\n",
        "    }\n",
        "    if 'token_type_ids' in batch:\n",
        "        out['token_type_ids'] = _jt_array(batch['token_type_ids'], 'int32')\n",
        "    return out\n",
        "\n",
        "def collate_mr(batch):\n",
        "    out = {\n",
        "        'input_ids': np.asarray([b['input_ids'] for b in batch], dtype=np.int32),\n",
        "        'attention_mask': np.asarray([b['attention_mask'] for b in batch], dtype=np.float32),\n",
        "        'labels': np.asarray([b['labels'] for b in batch], dtype=np.int32),\n",
        "    }\n",
        "    if 'token_type_ids' in batch[0]:\n",
        "        out['token_type_ids'] = np.asarray([b['token_type_ids'] for b in batch], dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "def prepare_mr_dataset(split, tokenizer, max_length=128):\n",
        "    ds = load_from_disk(os.path.join(data_dir, 'MR'))[split]\n",
        "    def tokenize_fn(batch):\n",
        "        tok = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=max_length)\n",
        "        tok['labels'] = batch['label']\n",
        "        return tok\n",
        "    return ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)\n",
        "\n",
        "tokenizer_dir = './hf/roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True)\n",
        "\n",
        "train_ds = prepare_mr_dataset('train', tokenizer)\n",
        "dev_ds = prepare_mr_dataset('validation', tokenizer)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_mr)\n",
        "dev_loader = DataLoader(dev_ds, batch_size=32, shuffle=False, collate_fn=collate_mr)\n",
        "\n",
        "model = SBERTJittor(\n",
        "    encoder_name=base_model,\n",
        "    pooling=pooling,\n",
        "    head_type='none',\n",
        "    checkpoint_path=None,\n",
        ")\n",
        "payload = jt.load(jittor_ckpt)\n",
        "if isinstance(payload, dict) and 'model_state' in payload:\n",
        "    model.load_state_dict(payload['model_state'])\n",
        "else:\n",
        "    model.load_state_dict(payload)\n",
        "\n",
        "classifier = nn.Linear(model.output_dim, 2)\n",
        "optimizer = nn.Adam(list(model.parameters()) + list(classifier.parameters()), lr=2e-5)\n",
        "loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = -1.0\n",
        "for epoch in range(3):\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
        "        jt_batch = _to_batch(batch)\n",
        "        reps = model.encode(jt_batch['input_ids'], jt_batch['attention_mask'], jt_batch.get('token_type_ids'))\n",
        "        logits = classifier(reps)\n",
        "        loss = loss_fct(logits, jt_batch['labels'])\n",
        "        optimizer.step(loss)\n",
        "        total_loss += loss.item() * jt_batch['labels'].shape[0]\n",
        "        total_samples += jt_batch['labels'].shape[0]\n",
        "    print(f'Epoch {epoch+1} train loss:', total_loss / max(total_samples, 1))\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with jt.no_grad():\n",
        "        for batch in dev_loader:\n",
        "            jt_batch = _to_batch(batch)\n",
        "            reps = model.encode(jt_batch['input_ids'], jt_batch['attention_mask'], jt_batch.get('token_type_ids'))\n",
        "            logits = classifier(reps)\n",
        "            preds = jt.argmax(logits, dim=1)[0]\n",
        "            correct += jt.sum(preds == jt_batch['labels']).item()\n",
        "            total += jt_batch['labels'].shape[0]\n",
        "    acc = correct / max(total, 1) * 100\n",
        "    print(f'Epoch {epoch+1} dev acc:', acc)\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        safe_model = base_model.replace('/', '_')\n",
        "        ckpt_path = os.path.join(output_dir, f'{safe_model}_best.pkl')\n",
        "        jt.save({'model_state': model.state_dict(), 'classifier_state': classifier.state_dict()}, ckpt_path)\n",
        "        print('Saved best checkpoint to', ckpt_path)\n",
        "\n",
        "print('Best dev acc:', best_acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ann_final",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
