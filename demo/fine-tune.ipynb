{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SBERT-Jittor Fine-tuning Demo\n",
        "\n",
        "This notebook loads data from Hugging Face datasets, fine-tunes on NLI, then runs STS regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_repo_root(start: Path):\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'model' / 'sbert_model.py').is_file():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "repo_root = _find_repo_root(Path.cwd())\n",
        "if repo_root is None:\n",
        "    print('SBERT_JITTOR root not found. Set sys.path manually.')\n",
        "else:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "    os.chdir(repo_root)\n",
        "    print(f'Using repo root: {repo_root}')\n",
        "\n",
        "# If you want to use code from the HF repo instead:\n",
        "# from huggingface_hub import snapshot_download\n",
        "# repo_dir = Path(snapshot_download('Kyle-han/roberta-base-nli-mean-tokens'))\n",
        "# sys.path.insert(0, str(repo_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import jittor as jt\n",
        "from argparse import Namespace\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from utils.download_data import download_nli_datasets, download_sts_benchmark\n",
        "from training.nli.train_nli import train as train_nli\n",
        "from training.sts.train_sts import train as train_sts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Data download (run once)\n",
        "data_dir = './data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "# download_nli_datasets(data_dir)\n",
        "# download_sts_benchmark(data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Tokenizer from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
        "tokenizer('hello world', return_tensors='np')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NLI fine-tuning\n",
        "nli_args = Namespace(\n",
        "    base_model='bert-base-uncased',\n",
        "    pooling='mean',\n",
        "    loss='softmax',\n",
        "    ablation=0,\n",
        "    encoder_checkpoint=None,\n",
        "    tokenizer_dir=None,\n",
        "    num_labels=3,\n",
        "    data_dir=data_dir,\n",
        "    datasets=['SNLI', 'MultiNLI'],\n",
        "    max_length=128,\n",
        "    batch_size=16,\n",
        "    eval_batch_size=32,\n",
        "    epochs=1,\n",
        "    lr=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    use_cuda=jt.has_cuda,\n",
        "    log_steps=100,\n",
        "    eval_steps=1000,\n",
        "    save_steps=0,\n",
        "    start_from_checkpoints=None,\n",
        "    output_dir=None,\n",
        "    num_workers=4,\n",
        "    cache_dir=None,\n",
        "    overwrite_cache=False,\n",
        "    tokenize_batch_size=1024,\n",
        ")\n",
        "\n",
        "train_nli(nli_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# STS regression fine-tuning\n",
        "sts_args = Namespace(\n",
        "    base_model='bert-base-uncased',\n",
        "    pooling='mean',\n",
        "    encoder_checkpoint=None,\n",
        "    tokenizer_dir=None,\n",
        "    data_dir=data_dir,\n",
        "    cache_dir=None,\n",
        "    overwrite_cache=False,\n",
        "    tokenize_batch_size=1024,\n",
        "    train_dataset='STS-B',\n",
        "    train_split='train',\n",
        "    eval_dataset='STS-B',\n",
        "    eval_split='validation',\n",
        "    test_dataset='STS-B',\n",
        "    test_split='test',\n",
        "    batch_size=32,\n",
        "    eval_batch_size=32,\n",
        "    epochs=1,\n",
        "    lr=2e-5,\n",
        "    max_length=128,\n",
        "    log_steps=20,\n",
        "    eval_steps=180,\n",
        "    save_steps=0,\n",
        "    disable_checkpoint=False,\n",
        "    num_workers=4,\n",
        "    use_cuda=jt.has_cuda,\n",
        "    normalize_scores=False,\n",
        "    score_scale=5.0,\n",
        "    start_from_checkpoints=None,\n",
        "    output_dir=None,\n",
        ")\n",
        "\n",
        "train_sts(sts_args)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}