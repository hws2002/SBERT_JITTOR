{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SBERT-Jittor Fine-tuning Demo\n",
        "\n",
        "Step-by-step NLI fine-tuning and STS regression without calling training helpers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_repo_root(start: Path):\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'model' / 'sbert_model.py').is_file():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "repo_root = _find_repo_root(Path.cwd())\n",
        "if repo_root is None:\n",
        "    print('SBERT_JITTOR root not found. Set sys.path manually.')\n",
        "else:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "    os.chdir(repo_root)\n",
        "    print(f'Using repo root: {repo_root}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "os.environ.setdefault('HF_HOME', './.hf_cache')\n",
        "os.environ.pop('TRANSFORMERS_CACHE', None)\n",
        "warnings.filterwarnings(\n",
        "    'ignore',\n",
        "    message='Using `TRANSFORMERS_CACHE` is deprecated',\n",
        "    category=FutureWarning,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import jittor as jt\n",
        "from jittor.dataset import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "from model.sbert_model import SBERTJittor\n",
        "from losses.softmax_loss import SoftmaxLoss\n",
        "from losses.regression_loss import RegressionLoss\n",
        "from utils.data_loader import prepare_nli_dataset, prepare_sts_dataset, collate_nli, collate_sts\n",
        "from utils.jt_utils import _to_jittor_batch, setup_device\n",
        "\n",
        "setup_device(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Data directory (download via utils/download_data.py first)\n",
        "data_dir = './data'\n",
        "cache_dir = None  # default -> data/_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NLI dataset + loader\n",
        "train_dataset = prepare_nli_dataset(\n",
        "    data_dir=data_dir,\n",
        "    datasets=['SNLI', 'MultiNLI'],\n",
        "    split='train',\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=128,\n",
        "    cache_dir=cache_dir,\n",
        "    overwrite_cache=False,\n",
        "    tokenize_batch_size=1024,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_batch=collate_nli,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NLI model + loss + optimizer\n",
        "model = SBERTJittor('bert-base-uncased', pooling='mean', head_type='none')\n",
        "train_loss = SoftmaxLoss(model=model, num_labels=3, ablation=0)\n",
        "optimizer = jt.optim.AdamW(train_loss.parameters(), lr=2e-5, weight_decay=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NLI training loop (1 epoch demo)\n",
        "steps_per_epoch = math.ceil(len(train_dataset) / 16)\n",
        "warmup_steps = max(int(steps_per_epoch * 0.1), 1)\n",
        "global_step = 0\n",
        "\n",
        "for step, batch in enumerate(train_loader, 1):\n",
        "    jt_batch = _to_jittor_batch(batch, for_sts=False)\n",
        "    labels = jt_batch['labels']\n",
        "    loss, logits = train_loss(jt_batch, labels)\n",
        "    optimizer.step(loss)\n",
        "\n",
        "    global_step += 1\n",
        "    if global_step <= warmup_steps:\n",
        "        optimizer.lr = 2e-5 * (global_step / warmup_steps)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        preds = jt.argmax(logits, dim=1)[0]\n",
        "        acc = (jt.sum(preds == labels).item() / labels.shape[0]) * 100\n",
        "        print(f'step {step} loss={loss.item():.4f} acc={acc:.2f}%')\n",
        "    if step >= steps_per_epoch:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# STS dataset + loader\n",
        "sts_dataset = prepare_sts_dataset(\n",
        "    data_dir=data_dir,\n",
        "    dataset_name='STS-B',\n",
        "    split='validation',\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=128,\n",
        "    cache_dir=cache_dir,\n",
        "    overwrite_cache=False,\n",
        "    tokenize_batch_size=1024,\n",
        ")\n",
        "\n",
        "sts_loader = DataLoader(\n",
        "    sts_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_batch=collate_sts,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# STS evaluation loop (explicit)\n",
        "all_preds = []\n",
        "all_scores = []\n",
        "model.eval()\n",
        "with jt.no_grad():\n",
        "    for batch in sts_loader:\n",
        "        jt_batch = _to_jittor_batch(batch, for_sts=True)\n",
        "        emb_a = model.encode(jt_batch['input_ids_a'], jt_batch['attention_mask_a'], jt_batch.get('token_type_ids_a'))\n",
        "        emb_b = model.encode(jt_batch['input_ids_b'], jt_batch['attention_mask_b'], jt_batch.get('token_type_ids_b'))\n",
        "        emb_a_np = emb_a.numpy()\n",
        "        emb_b_np = emb_b.numpy()\n",
        "        denom = np.linalg.norm(emb_a_np, axis=1) * np.linalg.norm(emb_b_np, axis=1) + 1e-9\n",
        "        sim = np.sum(emb_a_np * emb_b_np, axis=1) / denom\n",
        "        all_preds.extend(sim.tolist())\n",
        "        all_scores.extend(jt_batch['scores'].numpy().reshape(-1).tolist())\n",
        "\n",
        "pearson, _ = pearsonr(all_preds, all_scores)\n",
        "spearman, _ = spearmanr(all_preds, all_scores)\n",
        "print({'pearson': pearson * 100, 'spearman': spearman * 100})\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# STS regression fine-tuning (1 epoch demo)\n",
        "reg_loss = RegressionLoss()\n",
        "optimizer = jt.optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(sts_dataset) / 32)\n",
        "for step, batch in enumerate(sts_loader, 1):\n",
        "    jt_batch = _to_jittor_batch(batch, for_sts=True)\n",
        "    emb_a = model.encode(jt_batch['input_ids_a'], jt_batch['attention_mask_a'], jt_batch.get('token_type_ids_a'))\n",
        "    emb_b = model.encode(jt_batch['input_ids_b'], jt_batch['attention_mask_b'], jt_batch.get('token_type_ids_b'))\n",
        "    loss = reg_loss(emb_a, emb_b, jt_batch['scores'])\n",
        "    optimizer.step(loss)\n",
        "    if step % 100 == 0:\n",
        "        print(f'step {step} loss={loss.item():.4f}')\n",
        "    if step >= steps_per_epoch:\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}