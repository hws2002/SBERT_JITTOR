{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "869b5fff",
      "metadata": {},
      "source": [
        "# SBERT-Jittor Evaluation Demo\n",
        "\n",
        "Explicit evaluation loop (no helper functions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1242f116",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_repo_root(start: Path):\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'model' / 'sbert_model.py').is_file():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "repo_root = _find_repo_root(Path.cwd())\n",
        "if repo_root is None:\n",
        "    print('SBERT_JITTOR root not found. Set sys.path manually.')\n",
        "else:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "    os.chdir(repo_root)\n",
        "    print(f'Using repo root: {repo_root}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366c1706",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "os.environ.setdefault('HF_HOME', './.hf_cache')\n",
        "os.environ.pop('TRANSFORMERS_CACHE', None)\n",
        "warnings.filterwarnings(\n",
        "    'ignore',\n",
        "    message='Using `TRANSFORMERS_CACHE` is deprecated',\n",
        "    category=FutureWarning,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Explicit STS evaluation script\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import jittor as jt\n",
        "from jittor.dataset import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "from model.sbert_model import SBERTJittor\n",
        "from utils.data_loader import prepare_sts_dataset, collate_sts\n",
        "from utils.jt_utils import _to_jittor_batch, setup_device\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Explicit STS evaluation\")\n",
        "    parser.add_argument(\"--data_dir\", default=\"./data\", help=\"Dataset root\")\n",
        "    parser.add_argument(\"--dataset\", default=\"STS-B\", help=\"Dataset name\")\n",
        "    parser.add_argument(\"--split\", default=\"test\", help=\"Split name\")\n",
        "    parser.add_argument(\"--repo_id\", default=\"Kyle-han/roberta-base-nli-mean-tokens\",\n",
        "                        help=\"HF repo id containing tokenizer + Jittor checkpoint\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n",
        "    parser.add_argument(\"--max_length\", type=int, default=128, help=\"Max sequence length\")\n",
        "    parser.add_argument(\"--cache_dir\", default=None, help=\"Cache dir for tokenized data\")\n",
        "    parser.add_argument(\"--overwrite_cache\", action=\"store_true\", help=\"Overwrite cached tokenization\")\n",
        "    parser.add_argument(\"--tokenize_batch_size\", type=int, default=1024, help=\"Tokenize batch size\")\n",
        "    parser.add_argument(\"--use_cuda\", action=\"store_true\", help=\"Use CUDA if available\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    setup_device(args.use_cuda)\n",
        "\n",
        "    model, tokenizer, _ = SBERTJittor.from_pretrained(\n",
        "        args.repo_id,\n",
        "        return_tokenizer=True,\n",
        "    )\n",
        "\n",
        "    sts_dataset = prepare_sts_dataset(\n",
        "        data_dir=args.data_dir,\n",
        "        dataset_name=args.dataset,\n",
        "        split=args.split,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=args.max_length,\n",
        "        cache_dir=args.cache_dir,\n",
        "        overwrite_cache=args.overwrite_cache,\n",
        "        tokenize_batch_size=args.tokenize_batch_size,\n",
        "    )\n",
        "\n",
        "    sts_loader = DataLoader(\n",
        "        sts_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        collate_batch=collate_sts,\n",
        "    )\n",
        "\n",
        "    all_preds = []\n",
        "    all_scores = []\n",
        "    model.eval()\n",
        "\n",
        "    with jt.no_grad():\n",
        "        for batch in sts_loader:\n",
        "            jt_batch = _to_jittor_batch(batch, for_sts=True)\n",
        "            emb_a = model.encode(\n",
        "                jt_batch[\"input_ids_a\"],\n",
        "                jt_batch[\"attention_mask_a\"],\n",
        "                jt_batch.get(\"token_type_ids_a\"),\n",
        "            )\n",
        "            emb_b = model.encode(\n",
        "                jt_batch[\"input_ids_b\"],\n",
        "                jt_batch[\"attention_mask_b\"],\n",
        "                jt_batch.get(\"token_type_ids_b\"),\n",
        "            )\n",
        "\n",
        "            emb_a_np = emb_a.numpy()\n",
        "            emb_b_np = emb_b.numpy()\n",
        "            denom = np.linalg.norm(emb_a_np, axis=1) * np.linalg.norm(emb_b_np, axis=1) + 1e-9\n",
        "            sim = np.sum(emb_a_np * emb_b_np, axis=1) / denom\n",
        "\n",
        "            all_preds.extend(sim.tolist())\n",
        "            all_scores.extend(jt_batch[\"scores\"].numpy().reshape(-1).tolist())\n",
        "\n",
        "    pearson, _ = pearsonr(all_preds, all_scores)\n",
        "    spearman, _ = spearmanr(all_preds, all_scores)\n",
        "\n",
        "    print({\"pearson\": pearson * 100, \"spearman\": spearman * 100})\n",
        "    print(\"scores nan:\", np.isnan(sts_dataset.arrays[\"scores\"]).any())\n",
        "    print(\"preds nan:\", np.isnan(all_preds).any())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}